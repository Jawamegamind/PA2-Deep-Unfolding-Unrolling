{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d55066e-4ecc-4de2-83b6-1c9e19374681",
      "metadata": {
        "id": "6d55066e-4ecc-4de2-83b6-1c9e19374681"
      },
      "source": [
        "# Assignment 2\n",
        "The objective of this assignment is to introduce you to function specific neural networks. In otherwords, we will try and design neural networks based on insights from existing optimization algorithms. In the process, you will realize that while deep neural networks can, in theory, be generalized to learn arbitrary functions, design choices determine how easily and quickly they are able to learn those functions.\n",
        "\n",
        "Additionally, when the training process starts, the model has to learn a particular mapping with no prior hint about the kind of function it is looking for. By introducing a structure which correlates to optimization objectives, neural networks can effectively be helped in narrowing done the choice of functions and parameters to be learnt.\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c76668",
      "metadata": {},
      "source": [
        "Name: Jawad Saeed\n",
        "\n",
        "Roll Number: 25100094\n",
        "\n",
        "Name: Muhammad Saad Haroon\n",
        "\n",
        "Roll Number: 25100147\n",
        "\n",
        "Name: Daanish Uddin Khan\n",
        "\n",
        "Roll Number: 25100004\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5167e62-1f61-44af-8b11-a88a29c72b4a",
      "metadata": {
        "id": "a5167e62-1f61-44af-8b11-a88a29c72b4a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.linalg\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "np.random.seed(0)\n",
        "gen = torch.Generator().manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i5Yfx4XdEnMa",
      "metadata": {
        "id": "i5Yfx4XdEnMa"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533bacc6",
      "metadata": {},
      "source": [
        "# Task 1: Sparsity and Robustness\n",
        "\n",
        "Please add your solution to this task from the manual in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356abb7a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd3be33",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d945cd4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a3735edc",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3e6f6e76",
      "metadata": {},
      "source": [
        "# Task 2: Sparsity in Contrastive Models\n",
        "\n",
        "Please add your solution to this task from the manual in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f92f02",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e50ca6",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bfddcb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2278632",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd29cc44",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac79a24",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "facbb23b",
      "metadata": {},
      "source": [
        "# Task 3: ISTA for Sparse Vector Recovery\n",
        "\n",
        "## Section 1: Optimization\n",
        "\n",
        "We first start by considering a well formulated problem:\n",
        "\n",
        "Consider some noisy observations $\\mathbf{x} \\in \\mathbb{R}^{m}$ generated from the following process:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{x} = \\mathbf{Az} + \\mathbf{n}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{z} \\in \\mathbb{R}^{m}$ is our target signal, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is a linear operator representing the process by which the signal has been measured, and $\\mathbf{n} \\in \\mathbb{R}^{n}$ represents additive noise. Our objective is to recover $\\mathbf{x}$.\n",
        "\n",
        "\n",
        "This can be formulated as an optimization problem as follows:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{z}} ||\\mathbf{x} - \\mathbf{Az}||_{2}^{2}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is the well known, least squares regression problem and there are various methods to solve it. We make our problem more interesting by adding the constraint that $\\mathbf{x}$ is **sparse**. Noting that the $l_{0}$ norm measures sparsity, our new optimization problem is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{z}} ||\\mathbf{x} - \\mathbf{Az}||_{2}^{2} + \\lambda||\\mathbf{z}||_{0}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This problem is **not** convex and hence cannot be solved using conventional gradient based methods.\n",
        "\n",
        "We relax the $l_{0}$ norm to the $l_{1}$ norm in order to bring our problem in the domain of convex problems:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{z}} ||\\mathbf{x} - \\mathbf{Az}||_{2}^{2} + \\lambda||\\mathbf{z}||_{1}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "You may know this as lasso regression. Note that this problem is not solvable via gradient descent since the $l_{1}$ norm is not differentiable. Many solvers exist for this problem, but we shall take a different approach known as the **Iterative Soft Thresholding Algorithm (ISTA)**.\n",
        "\n",
        "We can write our optimization objective as a sum of two functions:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{z}}{h(\\mathbf{z}) = f(\\mathbf{z}) + g(\\mathbf{z})}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $f$ is convex and differentiable while $g$ is convex but not necessarily differentiable.\n",
        "\n",
        "Note that if $g$ was differentiable, we could use a gradient based approach (e.g. gradient descent) to minimize it. Our approach is to design an iterative algorithm such that at iteration $k$ where the algorithm is at point $\\mathbf{z}_k$, we minimize another convex function $m_{k}$ fulfilling the following conditions:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "m_{k}(\\mathbf{z}) &\\geq h(\\mathbf{z}) \\; \\; ∀ \\mathbf{z} \\\\\n",
        "m_{k}(\\mathbf{z}_{k}) &= h(\\mathbf{z}_{k})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Our update equation then becomes:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{z}_{k+1} = \\min_{\\mathbf{z}}{m_{k}({\\mathbf{z}})}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Coming back to our problem, we define the following function:\n",
        "$$\n",
        "\\begin{align}\n",
        "m_{k}(\\mathbf{z}) = ||\\mathbf{x} - \\mathbf{Az}||_{2}^{2} + λ||\\mathbf{z}||_{1} + (\\mathbf{z} - \\mathbf{z}_{k})^{T}(α\\mathbf{I} - \\mathbf{A}^{T}\\mathbf{A})(\\mathbf{x} - \\mathbf{z}_{k})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Note that at $\\mathbf{z} = \\mathbf{z}_{k}$, the additional quadratic term becomes 0 while for all other $\\mathbf{z}$, the additional term is positive as long as $α > \\max(\\text{eig}(\\mathbf{A}^{T}\\mathbf{A}))$ thereby satisfying the two conditions defined above. Each iterative step that minimizes this function, also minimizes the original objective function.\n",
        "The expression for $\\min_{\\mathbf{z}}m_{k}(\\mathbf{z}_{k})$ can be written as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{z}}{||\\mathbf{z}_{k} + \\frac{1}{\\alpha}\\mathbf{A}^{T}(\\mathbf{x} - \\mathbf{A}\\mathbf{z}_{k}) - \\mathbf{z}||_{2}^{2} + \\frac{λ}{\\alpha}||\\mathbf{z}||_{1}}\n",
        "\\end{align}\n",
        "$$\n",
        "While an exact solution for the miniimzation problem does not exist, by finding a minimizer at each $k$, we can approach the global minimizer of the original problem.\n",
        "\n",
        "Each update iteration looks as follows:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{z}_{k+1} = \\text{soft}(\\mathbf{z}_{k} + \\frac{1}{\\alpha}\\mathbf{A}(\\mathbf{x} - \\mathbf{A}^{T}\\mathbf{z}_{k}), \\frac{\\lambda}{2\\alpha})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{soft}(\\mathbf{x}, \\tau) := \\text{sign}(\\mathbf{x})\\max(0, |\\mathbf{x}| - \\tau)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The soft thresholding operator is also known as the proximal gradient operator for the $l_{1}$ norm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1p4nmJng2Jir",
      "metadata": {
        "id": "1p4nmJng2Jir"
      },
      "source": [
        "Consider the following example where $\\mathbf{z} \\in \\mathbb{R}^{100}$,  $\\mathbf{A} \\in \\mathbb{R}^{100 \\times 100}$ and $\\mathbf{n} \\sim \\mathcal{N}(0, 0.05^{2}\\mathbf{I}) \\in \\mathbb{R}^{100}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417ed5b3-693e-4a75-81d5-50862f5adf47",
      "metadata": {
        "id": "417ed5b3-693e-4a75-81d5-50862f5adf47"
      },
      "outputs": [],
      "source": [
        "t = np.arange(0, 100, 1)\n",
        "z = 1.5 * np.random.rand(len(t))\n",
        "z = z * np.array([0 if np.random.rand() < 0.8 else 1 for i in range(len(t))])\n",
        "\n",
        "A = np.random.rand(100, 100)\n",
        "\n",
        "sigma = 0.05\n",
        "noise = np.random.normal(0, sigma)\n",
        "x = A @ z + noise\n",
        "\n",
        "\n",
        "fig = plt.figure(1, dpi=100, figsize=(12, 5))\n",
        "ax  = fig.subplots(nrows=1, ncols=2)\n",
        "_   = ax[0].stem(t, z, \"k\", basefmt=\"ko\")\n",
        "_   = ax[0].set_xlabel(r'$t$')\n",
        "_   = ax[0].set_ylabel(r'$z$')\n",
        "_   = ax[0].set_title(\"Original Signal\")\n",
        "\n",
        "_   = ax[1].stem(t, x, \"b\", basefmt=\"bo\")\n",
        "_   = ax[1].set_xlabel(r'$t$')\n",
        "_   = ax[1].set_ylabel(r'$x$')\n",
        "_   = ax[1].set_title(\"Noisy Measurement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8HO19OhA0w8",
      "metadata": {
        "id": "V8HO19OhA0w8"
      },
      "source": [
        "### Task 3.1:\n",
        "\n",
        "Implement the ISTA class for a fixed number of iterations. For the given signal, plot the total loss and sparse loss as a function of iterations. Also plot the recovered and original signals on a $2 \\times 1$ grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDZpa1qt_Q5L",
      "metadata": {
        "id": "bDZpa1qt_Q5L"
      },
      "outputs": [],
      "source": [
        "class ISTA:\n",
        "    def __init__(self, lambd: float, max_iter=1000):\n",
        "        \"\"\"Initialize the ISTA algorithm parameters.\"\"\"\n",
        "        self.lambd = lambd\n",
        "        self.max_iter = max_iter\n",
        "        self.loss_history_reconstruction = []\n",
        "        self.loss_history_sparse = []\n",
        "        self.loss_history_combined = []\n",
        "\n",
        "    def _soft(self, z: np.ndarray, T: float) -> np.ndarray:\n",
        "        \"\"\"Apply soft-thresholding to the input vector.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def _run_one_iter(self, z: np.ndarray, x: np.ndarray, A: np.ndarray, alpha: float):\n",
        "        \"\"\"Run one iteration of the ISTA algorithm.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def optimize(self, A: np.ndarray, x: np.ndarray, alpha: float) -> np.ndarray:\n",
        "        \"\"\"Optimize the solution using the ISTA algorithm.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def visualize_loss_history(self):\n",
        "        \"\"\"Visualize the loss history over the iterations.\"\"\"\n",
        "        pass  # To be implemented\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UU9AQx02LCgL",
      "metadata": {
        "id": "UU9AQx02LCgL"
      },
      "outputs": [],
      "source": [
        "opt = ISTA(2.5, max_iter=5000)\n",
        "eigs, _ = np.linalg.eig(A.T @ A)\n",
        "alpha = max(eigs) + 100\n",
        "z_recovered = opt.optimize(A, x, alpha)\n",
        "opt.visualize_loss_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_MIulJHLMpI-",
      "metadata": {
        "id": "_MIulJHLMpI-"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(dpi=100, figsize=(15, 10))\n",
        "ax  = fig.subplots(2, 1)\n",
        "_   = ax[0].stem(t, x, \"k\", label=\"Original Signal\")\n",
        "_   = ax[0].set_xlabel(r'$t$')\n",
        "_   = ax[0].set_ylabel(r'$z$')\n",
        "_   = ax[0].set_title(\"Original Signal\")\n",
        "\n",
        "_   = ax[1].stem(t, z_recovered, \"b\", label=\"Recovered Signal\")\n",
        "_   = ax[1].set_xlabel(r'$t$')\n",
        "_   = ax[1].set_ylabel(r'$\\hat{z}$')\n",
        "_   = ax[1].set_title(\"Recovered Signal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b317d3",
      "metadata": {},
      "source": [
        "### Task 3.2\n",
        "\n",
        "Add in your response to Task 3.2 from the manual in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4cc81a5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "GDwuTIyiVgSc",
      "metadata": {
        "id": "GDwuTIyiVgSc"
      },
      "source": [
        "## Section 2: Unfolding\n",
        "\n",
        "In section 1, we had the luxury of knowing what our measurement matrix $A$ was. Yet, we had to choose values of $\\alpha$ and $\\lambda$ manually and a little experimentation will show you that their choice impacts the performance of the algorithm greatly. Our new problem setup assumes that we only have access to some noisy measurements $\\mathbf{z}$ and our objective is to recover the underlying sparse signal $\\mathbf{z}$ with no knowledge of $\\mathbf{A}$.\n",
        "\n",
        "To do so, we turn our attention towards neural networks.\n",
        "\n",
        "\n",
        "The following code will help you load and create a dataset from the given `sparse_signals.pkl` file. Do not change it.\n",
        "\n",
        "The training data comprises of 9000 input $\\mathbf{z} \\in \\mathbb{R}^{100}$ and output $\\mathbf{x} \\in \\mathbb{R}^{100}$ signal pairs while test set contains 1000 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KI_DFPt7TZaE",
      "metadata": {
        "id": "KI_DFPt7TZaE"
      },
      "outputs": [],
      "source": [
        "class SparseSignalsDataset(Dataset):\n",
        "  def __init__(self, Z, X):\n",
        "    self.z = torch.tensor(np.array(Z), dtype=torch.float32)\n",
        "    self.x = torch.tensor(np.array(X), dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.z)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.z[idx], self.x[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GN250kfy45DS",
      "metadata": {
        "id": "GN250kfy45DS"
      },
      "outputs": [],
      "source": [
        "def load_file(filename='signals.pkl'):\n",
        "    with open(filename, 'rb') as f:\n",
        "        Z, X = pickle.load(f)\n",
        "    return Z, X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edPMLvLq_kbU",
      "metadata": {
        "id": "edPMLvLq_kbU"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY\n",
        "filename = \"/content/sparse_signals.pkl\"\n",
        "Z, X = load_file(filename=filename)\n",
        "dataset = SparseSignalsDataset(Z, X)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "test_size = len(dataset) - train_size  # 20% for testing\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=gen)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cxJy_o-e_cet",
      "metadata": {
        "id": "cxJy_o-e_cet"
      },
      "source": [
        "### Task 3.3\n",
        "\n",
        "Refer to the manual for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9vdgsm5OAUPr",
      "metadata": {
        "id": "9vdgsm5OAUPr"
      },
      "outputs": [],
      "source": [
        "def visualize(model, test_loader, num_samples, configs):\n",
        "  model.eval()\n",
        "  fig = plt.figure(figsize=(12, 5))\n",
        "  axs = fig.subplots(2, num_samples)\n",
        "  t = np.arange(0, 100, 1)\n",
        "  with torch.no_grad():\n",
        "    for i, (z, x) in enumerate(test_loader):\n",
        "      if i == num_samples:\n",
        "        break\n",
        "      z, x = z.to(configs[\"device\"]), x.to(configs[\"device\"])\n",
        "      z_hat = model(x)[0, :].cpu().numpy()\n",
        "      z = z[0, :].cpu().numpy()\n",
        "      _   = axs[0, i].stem(t, z, \"k\")\n",
        "      _   = axs[0, i].set_xlabel(r'$t$')\n",
        "      _   = axs[0, i].set_ylabel(r'$z$')\n",
        "      _   = axs[0, i].set_title(\"Original Signal\")\n",
        "\n",
        "      _   = axs[1, i].stem(t, z_hat, \"b\")\n",
        "      _   = axs[1, i].set_xlabel(r'$t$')\n",
        "      _   = axs[1, i].set_ylabel(r'$\\hat{z}$')\n",
        "      _   = axs[1, i].set_title(\"Recovered Signal\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  return axs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4TKeFcOFQR5g",
      "metadata": {
        "id": "4TKeFcOFQR5g"
      },
      "outputs": [],
      "source": [
        "# Ignore the use of x, y here - those are just placeholders\n",
        "\n",
        "def train(model, train_loader, loss_fn, optimizer, configs):\n",
        "  model.train()\n",
        "  for epoch in range(configs[\"epochs\"]):\n",
        "    running_loss = 0.0\n",
        "    for x, y in train_loader:\n",
        "      x, y = x.to(configs[\"device\"]), y.to(configs[\"device\"])\n",
        "      optimizer.zero_grad()\n",
        "      x_hat = model(y)\n",
        "      loss = loss_fn(x_hat, x)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item() * y.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch: [{epoch+1}/{configs['epochs']}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, test_loader, loss_fn, configs, tol=1e-6):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  sparsities = []\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "      x, y = x.to(configs[\"device\"]), y.to(configs[\"device\"])\n",
        "      x_hat = model(y)\n",
        "      loss = loss_fn(x_hat, x)\n",
        "      running_loss += loss.item() * y.size(0)\n",
        "      sparsities.append(torch.sum(torch.abs(x_hat) < tol).item()/x_hat.numel())\n",
        "  avg_loss = running_loss / len(test_loader.dataset)\n",
        "  avg_sparsity = np.mean(sparsities)\n",
        "  print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "  print(f\"Average Sparsity: {avg_sparsity:.4f}\")\n",
        "  return avg_loss, avg_sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AbKynlX09exl",
      "metadata": {
        "id": "AbKynlX09exl"
      },
      "outputs": [],
      "source": [
        "class ISTABlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"Initialize the ISTA block with a learnable threshold and a linear layer.\"\"\"\n",
        "        super().__init__()\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def _soft(self, x, T):\n",
        "        \"\"\"Apply soft-thresholding with threshold T.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform the forward pass through the ISTA block.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "\n",
        "class UnfoldedNN(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        \"\"\"Initialize the unfolded neural network with multiple ISTA blocks.\"\"\"\n",
        "        super().__init__()\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform the forward pass through the unfolded neural network.\"\"\"\n",
        "        pass  # To be implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iP6md2ZaJXYi",
      "metadata": {
        "id": "iP6md2ZaJXYi"
      },
      "outputs": [],
      "source": [
        "configs_unfolded = {\n",
        "    \"input_dim\": 100,\n",
        "    \"hidden_dims\": [256, 256],\n",
        "    \"output_dim\": 100,\n",
        "    \"lr\": 1e-4,\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": 32,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mMcEQTy9JPT7",
      "metadata": {
        "id": "mMcEQTy9JPT7"
      },
      "outputs": [],
      "source": [
        "model_unfolded = UnfoldedNN(configs_unfolded).to(configs_unfolded[\"device\"])\n",
        "optimizer = Adam(model_unfolded.parameters(), lr=configs_unfolded[\"lr\"])\n",
        "L1 = nn.L1Loss()\n",
        "train(model=model_unfolded, train_loader=train_loader, loss_fn=L1, optimizer=optimizer, configs=configs_unfolded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55UE8mE5Jo4m",
      "metadata": {
        "id": "55UE8mE5Jo4m"
      },
      "outputs": [],
      "source": [
        "avg_eval_loss_unfolded, avg_eval_sparsity_unfolded = evaluate(model_unfolded, test_loader, L1, configs_unfolded)\n",
        "_ = visualize(model_unfolded, test_loader, 3, configs_unfolded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c6b562",
      "metadata": {},
      "source": [
        "### Task 3.4\n",
        "\n",
        "Add in your solution to Task 3.4 from the manual in the cells below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfb46e2",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "E-ug7Gvh4x46",
      "metadata": {
        "id": "E-ug7Gvh4x46"
      },
      "source": [
        "# Task 4: ISTA Extension to Matrices and Inclusion of Rank Constraints\n",
        "\n",
        "So far, we have been working with vectors and sparsity constraints. We now introduce two new challenges:\n",
        "\n",
        "1. Introducing rank constraints into our problem formulation.\n",
        "2. Extending our algorithms to matrices (can be used to deal with images, videos etc.)\n",
        "\n",
        "Note that the rank of a matrix describes the degree of correlation between its rows or columns. A low rank means that most of its rows or columns are similar. In the context of images, a low rank component could be used to represent a background (e.g. a grassy background would have large regions of highly similar pixel values). In the context of videos (assume that each frame is being treated as a vector of dimension $hw$ (height $\\times$ width) so that the columns of a matrix represent each frame), a low rank component could represent similarities between successive frames. In other words, such an approach could help us identify constant features and isolating things that change. This is used frequently in medical imaging techniques to isolate outliers from largely similar video/image frames that show blood or tissue.\n",
        "\n",
        "Mathematically, the rank of a matrix can be captured by the nuclear norm of a matrix $||\\mathbf{M}||_{*}$ which basically counts the number of non-zero singular values of $\\mathbf{M}$.\n",
        "The singular values are the non zero elements of the diagonal matrix $\\mathbf{\\Sigma}$ in the singular value decomposition (SVD) of $\\mathbf{M} = \\mathbf{U \\Sigma V}^{T}$.\n",
        "\n",
        "Our problem is that we are given some observation $\\mathbf{X} \\in \\mathbb{R}^{a \\times b}$ (an image or a video for example) and our objective is to recover a low rank component $\\mathbf{L} \\in \\mathbb{R}^{a \\times b}$ and sparse component $\\mathbf{S} \\in \\mathbb{R}^{a \\times b}$ such that $\\mathbf{X} = \\mathbf{AL} + \\mathbf{BS}$ where $\\mathbf{A}$ and $\\mathbf{B}$ represent some linear operations (this is similar to $\\mathbf{A}$ in the original least squares regression problem. Also note that in many cases it may be sufficient to for these to be identity matrices, but we shall generalize our formulation). Overall, our optimization problem is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\arg \\min_{\\mathbf{L, S}}{\\lambda_{1}||\\mathbf{L}||_{*} + \\lambda_{2}||\\mathbf{S}||_{1} + ||\\mathbf{X} - \\mathbf{AL} - \\mathbf{BS}||_{F}^{2}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is also known as Robust Principal Component Analysis (RPCA).\n",
        "\n",
        "Without going into more details here, the ISTA approach can be extended to this problem formulation via the following two update equations:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{L}_{k+1} &= \\text{svt}(\n",
        "(\\mathbf{I} - \\frac{1}{L_f} \\mathbf{A}^T \\mathbf{A}) \\mathbf{L}_k - \\mathbf{A}^T \\mathbf{B} \\mathbf{S}_k + \\mathbf{A}^T \\mathbf{X}, \\frac{\\lambda_1}{L_f}) \\\\\n",
        "\\mathbf{S}_{k+1} &= \\text{soft}((\\mathbf{I} - \\frac{1}{L_f} \\mathbf{B}^T \\mathbf{B}) \\mathbf{S}_k - \\mathbf{B}^T \\mathbf{A} \\mathbf{L}_k + \\mathbf{B}^T \\mathbf{X}, \\frac{\\lambda_2}{L_f})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "You have already seen the soft thresholding operator, the singular value thresholding operator is the proximal gradient operator for the nuclear norm and is defined as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{svt}(\\mathbf{X}, \\kappa) := \\mathbf{U} \\, \\max(\\mathbf{0}, \\mathbf{\\Sigma} - \\kappa\\mathbf{I}) \\, \\mathbf{V}^T\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{U \\Sigma V}^{T} = \\text{SVD}(\\mathbf{X})$\n",
        "\n",
        "Our focus is not on the details of this optimization algorithm. We shall instead focus on designing a neural network inspired by it.\n",
        "\n",
        "Note that for each update equation, we can replace the multiplicative matrices by learnable convolutional kernels and thresholds by learnable parameters so that each iteration (layer) looks as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{L}_{k+1} &= \\text{svt}(C_{1}^{k} * \\mathbf{L}_{k} + C_{2}^{k} * \\mathbf{S}_{k} + C_{3}^{k} * \\mathbf{X}, \\mu_{1})\\\\\n",
        "\\mathbf{S}_{k+1} &= \\text{soft}(K_{1}^{k} * \\mathbf{L}_{k} + K_{2}^{k} * \\mathbf{S}_{k} + K_{3}^{k} * \\mathbf{X}, \\mu_{2})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $C_{i}^{k}$ is the $i^\\text{th}$ convolutional kernel of layer $k$ in the $\\mathbf{L}$ network and $K_{i}^{k}$ is the $i^\\text{th}$ convolutional kernel of layer $k$ in the $\\mathbf{S}$ network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MEpC5PNRf5r8",
      "metadata": {
        "id": "MEpC5PNRf5r8"
      },
      "source": [
        "You will be working with the moving MNIST dataset for this problem. Each sample $\\mathbf{X} \\in \\mathbb{R}^{64 \\times 64}$ represents a frame from a video containing two mnist digits superimposed on a noisy, low rank background. The functions given below will help you load and visualize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VXuSvWQS9x6O",
      "metadata": {
        "id": "VXuSvWQS9x6O"
      },
      "outputs": [],
      "source": [
        "class MovingMNISTDataset(Dataset):\n",
        "    def __init__(self, L, S, X):\n",
        "        self.L = torch.tensor(np.array(L), dtype=torch.float32)\n",
        "        self.S = torch.tensor(np.array(S), dtype=torch.float32)\n",
        "        self.X = torch.tensor(np.array(X), dtype=torch.float32)\n",
        "\n",
        "        self.L = self.L.view(10, 1000, 64 * 64).permute(1, 2, 0)\n",
        "        self.S = self.S.view(10, 1000, 64 * 64).permute(1, 2, 0)\n",
        "        self.X = self.X.view(10, 1000, 64 * 64).permute(1, 2, 0)\n",
        "\n",
        "        self.L = self.L.unsqueeze(1)\n",
        "        self.S = self.S.unsqueeze(1)\n",
        "        self.X = self.X.unsqueeze(1)\n",
        "\n",
        "        print(\"L shape:\", self.L.shape)\n",
        "        print(\"S shape:\", self.S.shape)\n",
        "        print(\"X shape:\", self.X.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.L.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        L_seq = self.L[idx]\n",
        "        S_seq = self.S[idx]\n",
        "        X_seq = self.X[idx]\n",
        "\n",
        "        return L_seq, S_seq, X_seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pxWM_NVk-9u7",
      "metadata": {
        "id": "pxWM_NVk-9u7"
      },
      "outputs": [],
      "source": [
        "def load_mnist(filename='moving_mnist.pkl'):\n",
        "    with open(filename, 'rb') as f:\n",
        "        L, S, X = pickle.load(f)\n",
        "    return L, S, X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GFRsXy-n_JVv",
      "metadata": {
        "id": "GFRsXy-n_JVv"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY\n",
        "filename = \"/content/moving_mnist.pkl\"\n",
        "L, S, X = load_mnist(filename=filename)\n",
        "dataset = MovingMNISTDataset(L, S, X)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "test_size = len(dataset) - train_size  # 20% for testing\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=gen)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4V7cCjps7RV_",
      "metadata": {
        "id": "4V7cCjps7RV_"
      },
      "outputs": [],
      "source": [
        "# Display the shape of original data (X)\n",
        "print(\"Original X shape:\", X.shape)\n",
        "print(\"Original L shape:\", L.shape)\n",
        "print(\"Original S shape:\", S.shape)\n",
        "\n",
        "# Plot the first video frames of original X, L, and S\n",
        "def plot_video_frames(data, title):\n",
        "    video_idx = 0  # Selecting the first video (index 0)\n",
        "    frames = data[:, video_idx, :, :]  # Shape: (10, 64, 64)\n",
        "\n",
        "    # Create a figure with 10 subplots (one for each frame)\n",
        "    fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    for i in range(10):\n",
        "        axs[i].imshow(frames[i], cmap='gray')\n",
        "        axs[i].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot original X, L, and S\n",
        "print(\"\\nPlots for Original\")\n",
        "plot_video_frames(X, \"Original X (Original Video)\")\n",
        "plot_video_frames(L, \"Original L (Background)\")\n",
        "plot_video_frames(S, \"Original S (Foreground)\")\n",
        "\n",
        "# Simulating transformation via the dataset\n",
        "# L_new, S_new, X_new represent transformed data after the dataloader\n",
        "L_new, S_new, X_new = dataset[0]  # Dataset returns transformed tensors\n",
        "\n",
        "# Display the shape of transformed data (L_new, S_new, X_new)\n",
        "print(\"\\n\")\n",
        "print(\"Transformed L_new shape:\", L_new.shape)\n",
        "print(\"Transformed S_new shape:\", S_new.shape)\n",
        "print(\"Transformed X_new shape:\", X_new.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qpqzJlQ7f0_y",
      "metadata": {
        "id": "qpqzJlQ7f0_y"
      },
      "source": [
        "Now design a neural network with up to 5 layers based on the intuition for low rank and sparse solutions developed above. Each layer should contain two branches for performing proximal operations for the low rank and sparse component respectively.\n",
        "\n",
        "During training, plot the following:\n",
        "1. MSE Loss as a function of the number of epochs.\n",
        "2. Final values of learnt thresholding parameters for the L and S brances as a function of layer number.\n",
        "\n",
        "For the test data, plot the following two charts by averaging over all points:\n",
        "1. Low Rank:\n",
        "  - 3D plot showing layer number and index of singular value on the x and y axes and the singular values themselves on the z axis.\n",
        "\n",
        "2. Sparse:\n",
        "  - 2D plot showing layer number on the x axis and number of sparse values on the y axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6133d47e",
      "metadata": {
        "id": "6133d47e"
      },
      "outputs": [],
      "source": [
        "class ISTABlock(nn.Module):\n",
        "    def __init__(self, branch, kernel_size=3):\n",
        "        \"\"\"Initialize the ISTA block with a convolution layer and a learnable threshold.\"\"\"\n",
        "        super(ISTABlock, self).__init__()\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def _soft(self, X, thr):\n",
        "        \"\"\"Apply soft-thresholding to the input tensor.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def _svt(self, X, thr):\n",
        "        \"\"\"Apply Singular Value Thresholding (SVT) to the input tensor.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Perform the forward pass through the ISTA block based on the branch type.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "\n",
        "class UnfoldedDNN(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        \"\"\"Initialize the unfolded neural network with ISTA blocks for both 'S' and 'L' branches.\"\"\"\n",
        "        super(UnfoldedDNN, self).__init__()\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def get_thresholds(self):\n",
        "        \"\"\"Return the thresholds from both branches of the network.\"\"\"\n",
        "        pass  # To be implemented\n",
        "\n",
        "    def forward(self, D):\n",
        "        \"\"\"Perform the forward pass through the unfolded neural network and return activations.\"\"\"\n",
        "        pass  # To be implemented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B682K0GO6Efy",
      "metadata": {
        "id": "B682K0GO6Efy"
      },
      "source": [
        "Now Train the Model and evaluate the outputs. Complete the plotting functions and feel free to change the configurations or training settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xbYPbc18rZm",
      "metadata": {
        "id": "0xbYPbc18rZm"
      },
      "outputs": [],
      "source": [
        "def plot_mse_loss(epoch_losses):\n",
        "    \"\"\"\n",
        "    Plot MSE loss over epochs.\n",
        "    \"\"\"\n",
        "    pass  # To be implemented\n",
        "\n",
        "\n",
        "def plot_threshold_params(L_params, S_params):\n",
        "    \"\"\"\n",
        "    Plot threshold parameters for L and S branches across layers.\n",
        "    \"\"\"\n",
        "    pass  # To be implemented\n",
        "\n",
        "\n",
        "def get_singular_values(L_):\n",
        "    \"\"\"\n",
        "    Calculate singular values from the low-rank component.\n",
        "    \"\"\"\n",
        "    pass  # To be implemented\n",
        "\n",
        "\n",
        "def get_sparsity(S_, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Calculate sparsity (number of non-zero elements with tolerance 10^-6) from the sparse component.\n",
        "    \"\"\"\n",
        "    pass  # To be implemented\n",
        "\n",
        "\n",
        "def plot_singular_values_over_layers(singular_values):\n",
        "    \"\"\"\n",
        "    Plot singular values for each layer as a 3D surface plot.\n",
        "    \"\"\"\n",
        "    pass  # To be implemented\n",
        "\n",
        "\n",
        "def plot_sparse_values_over_layers(sparse_values):\n",
        "    \"\"\"\n",
        "    Plot average number of sparse values across layers.\n",
        "    \"\"\"\n",
        "    pass  # To be implemented\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xDQoNg1fc6DG",
      "metadata": {
        "id": "xDQoNg1fc6DG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_loader, loss_fn, optimizer, configs):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    L_params = []\n",
        "    S_params = []\n",
        "\n",
        "    for epoch in range(configs[\"epochs\"]):\n",
        "        running_loss = 0.0\n",
        "        with tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{configs['epochs']}]\", unit=\"batch\") as pbar:\n",
        "            for L, S, X in pbar:\n",
        "                L, S, X = L.to(configs[\"device\"]), S.to(configs[\"device\"]), X.to(configs[\"device\"])\n",
        "                optimizer.zero_grad()\n",
        "                _, (L_, S_) = model(X)\n",
        "                loss = loss_fn(L_, S_, L, S, X)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * L.size(0)\n",
        "                pbar.set_postfix(loss=running_loss / len(train_loader.dataset))\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "        thresholds_S, thresholds_L = model.get_thresholds()\n",
        "        L_params.append(thresholds_L)\n",
        "        S_params.append(thresholds_S)\n",
        "\n",
        "        print(f\"Epoch: [{epoch+1}/{configs['epochs']}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    plot_mse_loss(epoch_losses)\n",
        "    print(\"\\n\")\n",
        "    plot_threshold_params(L_params, S_params)\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, loss_fn, configs):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_sparsities = []\n",
        "    all_singular_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (L, S, X) in enumerate(test_loader):\n",
        "            L, S, X = L.to(configs[\"device\"]), S.to(configs[\"device\"]), X.to(configs[\"device\"])\n",
        "            (activations_L, activations_S), (L_ , S_)= model(X)\n",
        "            loss = loss_fn(L, S, L_, S_, X)\n",
        "            running_loss += loss.item() * L.size(0)\n",
        "\n",
        "            print(f\"Plot for random frame in {batch_idx}\")\n",
        "            plot_first_frame(L, S, X)\n",
        "\n",
        "            batch_singular_values = []\n",
        "            batch_sparsities = []\n",
        "            layer_idx = 0\n",
        "            for L_layer, S_layer in zip(activations_L, activations_S):\n",
        "                layer_idx += 1\n",
        "                singular_values = get_singular_values(L_layer)\n",
        "                sparsity = get_sparsity(S_layer)\n",
        "                batch_singular_values.append(singular_values)\n",
        "                batch_sparsities.append(sparsity)\n",
        "\n",
        "            all_singular_values.append(batch_singular_values)\n",
        "            all_sparsities.append(batch_sparsities)\n",
        "\n",
        "    avg_loss = running_loss / len(test_loader.dataset)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    avg_singular_values = np.mean(all_singular_values, axis=0)\n",
        "    avg_sparsities = np.mean(all_sparsities, axis=0)\n",
        "\n",
        "    plot_singular_values_over_layers(avg_singular_values)\n",
        "    print(\"\\n\")\n",
        "    plot_sparse_values_over_layers(avg_sparsities)\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def plot_first_frame(L, S, X):\n",
        "  L_frame_flat = L[0, 0, :, 0]  # Shape: (4096,)\n",
        "  S_frame_flat = S[0, 0, :, 0]  # Shape: (4096,)\n",
        "  X_frame_flat = X[0, 0, :, 0]  # Shape: (4096,)\n",
        "\n",
        "  L_frame = L_frame_flat.view(64, 64).cpu().numpy()\n",
        "  S_frame = S_frame_flat.view(64, 64).cpu().numpy()\n",
        "  X_frame = X_frame_flat.view(64, 64).cpu().numpy()\n",
        "\n",
        "  fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "  axs[0].imshow(L_frame, cmap='gray')\n",
        "  axs[0].set_title('Background (L)')\n",
        "  axs[0].axis('off')\n",
        "\n",
        "  axs[1].imshow(S_frame, cmap='gray')\n",
        "  axs[1].set_title('Foreground (S)')\n",
        "  axs[1].axis('off')\n",
        "\n",
        "  axs[2].imshow(X_frame, cmap='gray')\n",
        "  axs[2].set_title('Original (D)')\n",
        "  axs[2].axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def loss_fn(L_, S_, L, S, X):\n",
        "  loss_L = torch.nn.functional.mse_loss(L_, L)\n",
        "  loss_S = torch.nn.functional.mse_loss(S_, S)\n",
        "  return loss_L + loss_S\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb43510",
      "metadata": {
        "id": "dfb43510"
      },
      "outputs": [],
      "source": [
        "# Model and training configurations\n",
        "configs = {\n",
        "    \"num_layers\": 4,\n",
        "    \"kernel_size\": 3,\n",
        "    \"lr\": 1e-4,\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": 32,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Initialize the model, optimizer, and datasets\n",
        "model = UnfoldedDNN(configs).to(configs[\"device\"])\n",
        "optimizer = Adam(model.parameters(), lr=configs[\"lr\"])\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, loss_fn, optimizer, configs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6vtvCHusMpz1",
      "metadata": {
        "id": "6vtvCHusMpz1"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluate(model, test_loader, loss_fn, configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_eC3Z3Te6oVN",
      "metadata": {
        "id": "_eC3Z3Te6oVN"
      },
      "source": [
        "# Task 5: RPCA with Mixed Norms\n",
        "\n",
        "Add your solution to this task from the manual in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ab265e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5105533",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "174f6d93",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5397cc",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
